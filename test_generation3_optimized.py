#!/usr/bin/env python3
"""Generation 3 Optimization Test - Performance, Caching, and Concurrent Processing"""

import sys
import os
import time
import threading
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from quantum_planner import QuantumTaskPlanner, Agent, Task, Solution
from quantum_planner.performance import performance, LRUCache
from quantum_planner.concurrent_optimizer import concurrent_optimizer, ParallelDecomposer

def test_caching_performance():
    """Test caching mechanisms and performance improvements."""
    print("‚ö° Testing Caching Performance")
    
    # Clear any existing caches
    performance.clear_all_caches()
    
    planner = QuantumTaskPlanner(backend="auto", fallback="simulated_annealing")
    
    # Create a problem that should be cacheable
    agents = [
        Agent(id="agent1", skills=["python"], capacity=2),
        Agent(id="agent2", skills=["javascript"], capacity=2),
    ]
    
    tasks = [
        Task(id="task1", required_skills=["python"], priority=5, duration=2),
        Task(id="task2", required_skills=["javascript"], priority=3, duration=1),
    ]
    
    # First solve - should be cached
    start_time = time.time()
    solution1 = planner.assign(agents, tasks)
    first_solve_time = time.time() - start_time
    
    # Second solve - should use cache
    start_time = time.time()
    solution2 = planner.assign(agents, tasks)
    second_solve_time = time.time() - start_time
    
    print(f"üïê First solve time: {first_solve_time:.4f}s")
    print(f"üïê Second solve time: {second_solve_time:.4f}s")
    
    # Verify solutions are equivalent
    assert solution1.assignments == solution2.assignments, "Cached solution differs from original"
    
    # Second solve should be faster (cache hit)
    if solution2.metadata.get("cache_hit"):
        print("‚úÖ Cache hit detected on second solve")
    
    # Check cache statistics
    cache_stats = performance.get_performance_stats()
    print(f"üìä Cache statistics: {cache_stats['caches']}")
    
    print("‚úÖ Caching performance test passed")
    return True

def test_lru_cache():
    """Test LRU cache implementation."""
    print("\nüîÑ Testing LRU Cache")
    
    # Create a small cache for testing
    cache = LRUCache(max_size=3, ttl_seconds=5.0, max_memory_mb=1.0)
    
    # Test basic operations
    cache.put("key1", "value1")
    cache.put("key2", "value2") 
    cache.put("key3", "value3")
    
    assert cache.get("key1") == "value1", "Cache get failed"
    assert cache.get("key2") == "value2", "Cache get failed"
    assert cache.get("key3") == "value3", "Cache get failed"
    
    # Test eviction (add 4th item to size-3 cache)
    cache.put("key4", "value4")
    
    # key1 should be evicted (LRU)
    assert cache.get("key1") is None, "LRU eviction failed"
    assert cache.get("key4") == "value4", "New item not cached"
    
    # Test TTL
    cache.put("temp_key", "temp_value")
    time.sleep(0.1)  # Wait a bit
    
    # Should still be there
    assert cache.get("temp_key") == "temp_value", "Item expired too early"
    
    # Test statistics
    stats = cache.get_stats()
    assert stats['size'] > 0, "Cache stats not working"
    assert stats['hits'] > 0, "Hit count not working"
    
    print(f"üìà Cache stats: {stats}")
    print("‚úÖ LRU cache test passed")
    return True

def test_problem_analysis():
    """Test memoized problem analysis."""
    print("\nüîç Testing Problem Analysis")
    
    agents = [
        Agent(id="python_dev", skills=["python", "ml"], capacity=3),
        Agent(id="js_dev", skills=["javascript", "react"], capacity=2),
        Agent(id="devops", skills=["docker", "kubernetes"], capacity=2),
    ]
    
    tasks = [
        Task(id="ml_model", required_skills=["python", "ml"], priority=8, duration=4),
        Task(id="web_ui", required_skills=["javascript", "react"], priority=6, duration=3),
        Task(id="deployment", required_skills=["docker"], priority=7, duration=2),
    ]
    
    # Analyze problem
    analysis = performance.memoize_problem_analysis(agents, tasks)
    
    print(f"üìä Problem analysis: {analysis}")
    
    # Verify analysis components
    assert "complexity_score" in analysis, "Missing complexity score"
    assert "skill_diversity" in analysis, "Missing skill diversity"
    assert "load_balance_potential" in analysis, "Missing load balance potential"
    assert "optimal_backend" in analysis, "Missing backend suggestion"
    
    # Complexity should be reasonable
    assert 0 <= analysis["complexity_score"] <= 100, "Complexity score out of range"
    
    # Skill diversity should be reasonable
    assert 0 <= analysis["skill_diversity"] <= 1, "Skill diversity out of range"
    
    print("‚úÖ Problem analysis test passed")
    return True

def test_concurrent_optimization():
    """Test concurrent optimization capabilities."""
    print("\nüîÄ Testing Concurrent Optimization")
    
    # Ensure optimizer is running
    if not concurrent_optimizer.running:
        concurrent_optimizer.start()
    
    # Create multiple small problems
    problems = []
    for i in range(3):
        agents = [Agent(id=f"agent_{i}_1", skills=["python"], capacity=1)]
        tasks = [Task(id=f"task_{i}_1", required_skills=["python"], priority=1, duration=1)]
        problems.append((agents, tasks))
    
    # Submit jobs concurrently
    job_ids = []
    start_time = time.time()
    
    for i, (agents, tasks) in enumerate(problems):
        job_id = concurrent_optimizer.submit_job(
            agents=agents,
            tasks=tasks,
            objective="minimize_makespan",
            priority=i
        )
        job_ids.append(job_id)
        print(f"üì§ Submitted job {job_id}")
    
    # Wait for all jobs to complete
    solutions = []
    for job_id in job_ids:
        try:
            solution = concurrent_optimizer.get_result(job_id, timeout=10.0)
            solutions.append(solution)
            print(f"‚úÖ Job {job_id} completed")
        except TimeoutError:
            print(f"‚è∞ Job {job_id} timed out")
    
    total_time = time.time() - start_time
    print(f"üïê Total concurrent execution time: {total_time:.2f}s")
    
    # Check that we got solutions
    assert len(solutions) > 0, "No solutions received from concurrent processing"
    
    # Check queue statistics
    queue_stats = concurrent_optimizer.get_queue_stats()
    print(f"üìä Queue stats: {queue_stats}")
    
    print("‚úÖ Concurrent optimization test passed")
    return True

def test_parallel_decomposition():
    """Test parallel problem decomposition."""
    print("\nüì¶ Testing Parallel Decomposition")
    
    # Create a problem that can be decomposed
    agents = [
        Agent(id="python_dev1", skills=["python"], capacity=2),
        Agent(id="python_dev2", skills=["python"], capacity=2),
        Agent(id="js_dev1", skills=["javascript"], capacity=2),
        Agent(id="js_dev2", skills=["javascript"], capacity=2),
    ]
    
    tasks = [
        Task(id="python_task1", required_skills=["python"], priority=5, duration=2),
        Task(id="python_task2", required_skills=["python"], priority=4, duration=1),
        Task(id="js_task1", required_skills=["javascript"], priority=6, duration=3),
        Task(id="js_task2", required_skills=["javascript"], priority=3, duration=2),
    ]
    
    # Check if problem can be decomposed
    can_decompose = ParallelDecomposer.can_decompose(agents, tasks)
    print(f"üîç Can decompose: {can_decompose}")
    
    if can_decompose:
        # Decompose the problem
        subproblems = ParallelDecomposer.decompose(agents, tasks)
        print(f"üì¶ Created {len(subproblems)} subproblems")
        
        # Verify decomposition
        assert len(subproblems) > 1, "Decomposition should create multiple subproblems"
        
        # Check that all agents and tasks are covered
        all_agents_in_subproblems = set()
        all_tasks_in_subproblems = set()
        
        for sub_agents, sub_tasks in subproblems:
            for agent in sub_agents:
                all_agents_in_subproblems.add(agent.agent_id)
            for task in sub_tasks:
                all_tasks_in_subproblems.add(task.task_id)
        
        print(f"üìä Agents in subproblems: {all_agents_in_subproblems}")
        print(f"üìä Tasks in subproblems: {all_tasks_in_subproblems}")
        
        print("‚úÖ Parallel decomposition test passed")
    else:
        print("‚ÑπÔ∏è  Problem not suitable for decomposition (expected for small test)")
    
    return True

def test_memory_management():
    """Test memory management and cleanup."""
    print("\nüßπ Testing Memory Management")
    
    planner = QuantumTaskPlanner(backend="auto", fallback="simulated_annealing")
    
    # Create many small problems to fill cache
    initial_stats = performance.get_performance_stats()
    print(f"üìä Initial memory: {initial_stats.get('memory_usage', {})}")
    
    # Generate many problems
    for i in range(10):
        agents = [Agent(id=f"agent_{i}", skills=["python"], capacity=1)]
        tasks = [Task(id=f"task_{i}", required_skills=["python"], priority=1, duration=1)]
        
        # This should create cache entries
        solution = planner.assign(agents, tasks)
        assert solution is not None, f"Solution {i} failed"
    
    # Check cache growth
    after_stats = performance.get_performance_stats()
    print(f"üìä After filling cache: {after_stats['caches']}")
    
    # Clear caches
    planner.clear_performance_caches()
    
    # Check cache cleanup
    cleanup_stats = performance.get_performance_stats()
    print(f"üìä After cleanup: {cleanup_stats['caches']}")
    
    # Verify caches were cleared
    for cache_name, cache_stats in cleanup_stats['caches'].items():
        assert cache_stats['size'] == 0, f"Cache {cache_name} not properly cleared"
    
    print("‚úÖ Memory management test passed")
    return True

def test_performance_monitoring():
    """Test performance monitoring and statistics."""
    print("\nüìà Testing Performance Monitoring")
    
    planner = QuantumTaskPlanner(backend="auto", fallback="simulated_annealing")
    
    # Perform several operations
    agents = [Agent(id="agent1", skills=["python"], capacity=1)]
    tasks = [Task(id="task1", required_skills=["python"], priority=1, duration=1)]
    
    for i in range(3):
        solution = planner.assign(agents, tasks)
        assert solution is not None, f"Assignment {i} failed"
    
    # Get comprehensive performance stats
    perf_stats = planner.get_performance_stats()
    
    print(f"üìä Performance statistics:")
    print(f"   Optimization enabled: {perf_stats['optimization_enabled']}")
    print(f"   Caches: {list(perf_stats['caches'].keys())}")
    print(f"   Thread pools: {list(perf_stats['thread_pools'].keys())}")
    
    # Verify we have cache statistics
    assert 'caches' in perf_stats, "Missing cache statistics"
    assert len(perf_stats['caches']) > 0, "No cache statistics available"
    
    # Check for expected caches
    expected_caches = ['solutions', 'problem_analysis']
    for cache_name in expected_caches:
        assert cache_name in perf_stats['caches'], f"Missing {cache_name} cache"
    
    print("‚úÖ Performance monitoring test passed")
    return True

def test_scalability_simulation():
    """Test scalability with larger problems."""
    print("\nüöÄ Testing Scalability Simulation")
    
    planner = QuantumTaskPlanner(backend="auto", fallback="simulated_annealing")
    
    # Test different problem sizes
    problem_sizes = [(2, 3), (3, 5), (4, 6)]
    
    for num_agents, num_tasks in problem_sizes:
        print(f"üìä Testing problem size: {num_agents} agents, {num_tasks} tasks")
        
        # Create agents
        agents = [
            Agent(id=f"agent_{i}", skills=["python", "javascript"][i % 2:i % 2 + 1], capacity=2)
            for i in range(num_agents)
        ]
        
        # Create tasks
        tasks = [
            Task(id=f"task_{i}", required_skills=["python", "javascript"][i % 2:i % 2 + 1], 
                 priority=i + 1, duration=1)
            for i in range(num_tasks)
        ]
        
        # Measure solve time
        start_time = time.time()
        solution = planner.assign(agents, tasks)
        solve_time = time.time() - start_time
        
        print(f"   ‚è±Ô∏è  Solve time: {solve_time:.4f}s")
        print(f"   üìà Makespan: {solution.makespan}")
        print(f"   üí∞ Cost: {solution.cost}")
        
        # Verify solution
        assert len(solution.assignments) > 0, "No assignments generated"
        assert solution.makespan > 0, "Invalid makespan"
        
        # Check if caching improves second run
        start_time = time.time()
        solution2 = planner.assign(agents, tasks)
        cached_solve_time = time.time() - start_time
        
        print(f"   üîÑ Cached solve time: {cached_solve_time:.4f}s")
        
        if solution2.metadata.get("cache_hit"):
            print("   ‚úÖ Cache hit on second solve")
    
    print("‚úÖ Scalability simulation test passed")
    return True

if __name__ == "__main__":
    print("‚ö° Starting Generation 3 Optimization Tests\n")
    
    try:
        # Run all optimization tests
        test_lru_cache()
        test_caching_performance()
        test_problem_analysis()
        test_concurrent_optimization()
        test_parallel_decomposition()
        test_memory_management()
        test_performance_monitoring()
        test_scalability_simulation()
        
        print("\nüéâ ALL GENERATION 3 OPTIMIZATION TESTS PASSED!")
        print("‚úÖ LRU caching implemented and working")
        print("‚úÖ Solution caching providing performance benefits")
        print("‚úÖ Problem analysis with memoization")
        print("‚úÖ Concurrent optimization capabilities")
        print("‚úÖ Parallel problem decomposition")
        print("‚úÖ Memory management and cleanup")
        print("‚úÖ Performance monitoring and statistics")
        print("‚úÖ Scalability improvements demonstrated")
        
        # Final performance summary
        perf_stats = performance.get_performance_stats()
        print(f"\nüìä Final Performance Summary:")
        print(f"   Active caches: {len(perf_stats['caches'])}")
        print(f"   Thread pools: {len(perf_stats['thread_pools'])}")
        
        # Stop concurrent optimizer
        concurrent_optimizer.stop()
        
    except Exception as e:
        print(f"\n‚ùå OPTIMIZATION TEST FAILED: {e}")
        import traceback
        traceback.print_exc()
        concurrent_optimizer.stop()
        sys.exit(1)